{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/GoldilocksAI.png)\n",
    "# The Right Fit\n",
    "Much of machine learning is trying to *optimize* models, or find the perfect fit. You will learn more about what a model being *overfit* or *underfit* means below but ML engineers must constantly tune their models to find the perfect balance between the two, the so-called \"Goldilocks zone\". Observe the graph below:\n",
    "![](https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/OverfitAI.png)\n",
    "\n",
    "When designing models ML engineers look for the right fit, the lowest point on the red curve which indicates the lowest average error in their model. Optimizing ML models is important because just like clothes or uncomfortable beds a model that isn't properly optimized doesn't do its job very well. Therefore, one of the important skills to have as a ML engineer is to be able to identify inaccuricies within ML models and to optimize them to increase performance and find the perfect fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting v. Underfitting\n",
    "Overfitting is where a model matches the training data almost perfectly, so when the model is being created, it looks like a near 100% accuracy rate; however, it does poorly in validation and other new data because it is perfectly fitted to the data that it was trained on. Underfitting is the opposite, where a when the model is being built and tested, the accuracies are very low because the model is missing crucial categories that define trends in the data.\n",
    "\n",
    "Neither are desirable in a model as it greatly reduces the accuracy of the model when it is used on training data or a larger set of data that was not used for the initial creation of the model. This eliminates the model's usefulness as it will make inaccurate predictions about what it is processing because it is missing a broader understanding of trends/patterns in the data.\n",
    "\n",
    "For example, a model built using a basic decision tree to determine house prices like the one pictured created below could be overfit if it had so many end nodes that they were each incredibly specific (i.e. 1111 Sherwood Street will sell for \\\\$111,000). When this model is used on data it has never seen before it performs poorly because it was so specified to the data it was trained on. This leads to accuracy suffering as a whole. Conversely, a decision tree with only two end nodes (say \\\\$100,000 and \\\\$200,000 based on East or West Coast) would be inaccurate because it can't recognize broader trends in the data and is focused on only one or two features. Neither makes for a very good ML model. In this lesson you will learn about identifying overfitting/underfitting and how to correct it.\n",
    "\n",
    "![](https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/R3ywQsR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of overfitting can be seen in the model below. When you test the model purely on the data it was trained with it performs extremely well (extremely close estimates to actual housing costs). However, when tested on separate validation data the model has an extremely large margin of error. Below we will explore ways to help identify and correct overfitting and underfitting in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mean Average Error (MAE): $73\n",
      "Validation Mean Average Error (MAE): $32,978\n"
     ]
    }
   ],
   "source": [
    "# Code you have previously used to load data\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# Path of the file to read\n",
    "iowa_file_path = 'https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/train.csv'\n",
    "\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "# Create target object and call it y\n",
    "y = home_data.SalePrice\n",
    "# Create X\n",
    "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "X = home_data[features]\n",
    "\n",
    "# Split into validation and training data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Specify Model\n",
    "iowa_model = DecisionTreeRegressor(max_leaf_nodes = 10000)\n",
    "# Fit Model\n",
    "iowa_model.fit(train_X, train_y)\n",
    "\n",
    "# Make training predictions and calculate mean absolute error\n",
    "train_predictions = iowa_model.predict(train_X)\n",
    "train_mae = mean_absolute_error(train_y, train_predictions)\n",
    "print(\"Training Mean Average Error (MAE): ${:,.0f}\".format(train_mae))\n",
    "\n",
    "# Make validation predictions and calculate mean absolute error\n",
    "val_predictions = iowa_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_y, val_predictions)\n",
    "print(\"Validation Mean Average Error (MAE): ${:,.0f}\".format(val_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Analysis\n",
    "One way to identify overfitting/underfitting is if your model performs great with training data but poorly with new testing data which is why it is important to have both a large training dataset and a large testing dataset. One of the primary ways ML engineers work to address this problem is through something called hyperparameter analysis. Essentially this means trying a bunch of different values for various parameters within a model (such as the number of end nodes). This can either be done as a semi-automated process (like we will below) or as a fully automated process using services like AWS Sagemaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "We have created the function `get_mae` for you which will generate the MAE (mean average error) for different versions of our model. Code for this function can be viewed in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compare Different Tree Sizes\n",
    "Use the widget below to adjust the number of leaves in the final model or the `max_leaf_nodes`. Try to find the optimal value for minimizing inaccuracy. This will be the lowest value because the number displayed represents the average error and by minimizing error we improve accuracy. Remember that we are measuring MAE against our *testing* data, this allows us to determine what values minimize underfitting/overfitting and maximize the accuracy of our model when it is actually deployed in a real-world application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbafe818c3f74846ae15297859bb5017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='Number of Leaves:', max=200, min=2, step=10, style=Slideâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider\n",
    "def test(value):\n",
    "    print(get_mae(value, train_X, val_X, train_y, val_y))\n",
    "style = {'description_width': 'initial'}\n",
    "interact(test, value=widgets.IntSlider(min=2, max=200, step=10, description='Number of Leaves:', style=style, readout=True, continuous_update=True));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop tries the following values for `max_leaf_nodes` from a set of possible values. By trying a large variety of values and comparing the MAE the loop is able to identify what parameter minimizes inaccuracy. We will then print the optimal tree size and store it in the variable *best_tree_size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_leaf_nodes = 82\n"
     ]
    }
   ],
   "source": [
    "candidate_max_leaf_nodes = list(range(2, 1000))\n",
    "scores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}\n",
    "best_tree_size = min(scores, key=scores.get)\n",
    "print('Best max_leaf_nodes = ' + str(best_tree_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Fit Model Using All Data\n",
    "You know the best tree size. If you were going to deploy this model in practice, you would make it even more accurate by using all of the data and keeping that tree size.  That is, you don't need to hold out the validation data now that you've made all your modeling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Mean Average Error (MAE): $17,776\n"
     ]
    }
   ],
   "source": [
    "final_model = DecisionTreeRegressor(max_leaf_nodes = best_tree_size, random_state = 0)\n",
    "final_model.fit(X,y)\n",
    "cmbd_predictions = final_model.predict(X)\n",
    "cmbd_mae = mean_absolute_error(y, cmbd_predictions)\n",
    "print(\"Combined Mean Average Error (MAE): ${:,.0f}\".format(cmbd_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "While error certainly exists with just a few simple steps we have managed to reduce the error by almost \\$14,644 which is a significant margin. In addition, that was all accomplished by changing only one parameter of many. Hyperparameter analysis is an essential tool in any ML engineer's toolkit because it allows for the identification and minimization of underfitting and overfitting which can result in significant penalalties to accuracy when actually deployed in the real world.\n",
    "\n",
    "### Great Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
