{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/GoldilocksAI.png></center>\n",
    "\n",
    "Much of machine learning is trying to *optimize* models, or find the perfect fit.\n",
    "<center> <img src =https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/OverfitAI.png></center>\n",
    "\n",
    "When designing models ML engineers look for the right fit, the lowest point on the red curve which indicates the lowest average error in their model. Optimizing ML models is important because just like clothes or uncomfortable beds a model that isn't properly optimized doesn't do its job very well. Therefore, one of the important skills to have as a ML engineer is to be able to identify inaccuricies within ML models and to optimize them to increase performance and find the perfect fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our model...\n",
    "We got pretty good accuracy on our model. However, since when do we settle for \"pretty good\". In AI it's all about optimization. You'll learn about hyperparameter tuning as a way to decrease overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_leaf_nodes=10000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial Setup Code\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "file_path = 'https://raw.githubusercontent.com/UncertainQubit/firstrepo/master/train.csv'\n",
    "\n",
    "titanic_data = pd.read_csv(file_path)\n",
    "\n",
    "titanic_data = titanic_data.replace(to_replace='male', value='0', regex=True)\n",
    "titanic_data = titanic_data.replace(to_replace='fe0', value='1', regex=True)\n",
    "\n",
    "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']\n",
    "\n",
    "titanic_data = titanic_data[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch']].dropna()\n",
    "\n",
    "y = titanic_data.Survived\n",
    "\n",
    "X = titanic_data[features]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)\n",
    "\n",
    "model = DecisionTreeClassifier(max_leaf_nodes = 10000)\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying overfitting\n",
    "Overfitting is when a model isn't generalized enough and is instead closely fit to training data, giving it high accuracy on training data but low accuracy on data the model hasn't seen before. As you can see below, this model is slightly overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 94.018692%\n",
      "Validation Accuracy: 77.094972%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on training and validation data and calculate accuracy\n",
    "accuracy = model.score(train_X, train_y)\n",
    "print('Training Accuracy: {0:%}'.format(accuracy))\n",
    "val_accuracy = model.score(val_X, val_y)\n",
    "print('Validation Accuracy: {0:%}'.format(val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we reduce overfitting?\n",
    "One method is hyperparameter tuning, changing a number of variables that can affect our model. At industrial levels thousands of variables may be computationally modified but we'll just be changing two variables in our model today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37f32cfdc664763818adbbe625da805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='Number of leaves:', max=1000, min=2, step=10, style=Slidâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider\n",
    "def test(leafs, minsplit):\n",
    "    model = DecisionTreeClassifier(max_leaf_nodes = leafs, min_samples_split = minsplit, random_state = 0)\n",
    "    model.fit(train_X, train_y)\n",
    "    accuracy = model.score(val_X, val_y)\n",
    "    print('Accuracy: {0:%}'.format(accuracy))\n",
    "style = {'description_width': 'initial'}\n",
    "interact(\n",
    "    test, leafs=widgets.IntSlider(min=2, max=1000, step=10, description='Number of leaves:', style=style, readout=True, continuous_update=True),\n",
    "    minsplit=widgets.IntSlider(min=2, max=100, step=1, description='Minimum samples:', style=style, readout=True, continuous_update=True)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we make this easier?\n",
    "We don't have to do everyhting manually. With a little bit of code we can make a function that determines what number of maximum leaf nodes leads to the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(leaf_size):\n",
    "    model = DecisionTreeClassifier(max_leaf_nodes = leaf_size, min_samples_split = 2, random_state = 0)\n",
    "    model.fit(train_X, train_y)\n",
    "    return model.score(val_X, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best max_leaf_nodes = 20\n"
     ]
    }
   ],
   "source": [
    "candidate_max_leaf_nodes = list(range(2, 1000))\n",
    "scores = {leaf_size: score(leaf_size) for leaf_size in candidate_max_leaf_nodes}\n",
    "best_tree_size = max(scores, key=scores.get)\n",
    "print('Best max_leaf_nodes = ' + str(best_tree_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How much have we improved?\n",
    "Now that we've solved for some of the overfitting issues in our model let's train on ALL the data, not just the training data and use our optimal number of maximum leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 86.592179%\n"
     ]
    }
   ],
   "source": [
    "final_model = DecisionTreeClassifier(max_leaf_nodes = best_tree_size, min_samples_split = 2, random_state = 0)\n",
    "final_model.fit(X,y)\n",
    "\n",
    "final_accuracy = final_model.score(val_X, val_y)\n",
    "print('Validation Accuracy: {0:%}'.format(final_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement in Accuracy: 9.497207%\n"
     ]
    }
   ],
   "source": [
    "print('Improvement in Accuracy: {0:%}'.format(final_accuracy - val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wow!\n",
    "That's a pretty significant improvement. You're well on your way to designing professional AI models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
